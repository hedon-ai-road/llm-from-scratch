{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bde110",
   "metadata": {},
   "source": [
    "## 5-1 文本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "897a999d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from gpt_model import GPTModel, GPT_CONFIG_124M, token_ids_to_text, text_to_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440f7aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "inputs = torch.tensor([[16833, 3626, 6100], # [\"every effort moves\",\n",
    "                                        [40, 1107, 588]])  # \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345],  # [\" effort moves you\",\n",
    "                                      [1107, 588, 11311]]) # \" really like chocolate\"]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)  # 批次大小, 每行词元数量, 嵌入维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8387b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ffd5ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "    f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fe244",
   "metadata": {},
   "source": [
    "## 5-2 文本评估\n",
    "\n",
    "模型训练的目标是增大与正确目标词元 ID 对应的索引位置的 softmax 概率。\n",
    "\n",
    "1. 模型接收 3 个输入词元并生成 3 个向量\n",
    "2. 模型生成的张量中的每个向量的索引位置都对应词汇表中的一个单词\n",
    "3. 一个未训练的模型会为每个词元生成一个随机向量\n",
    "4. 模型训练时，目标是最大化与目标向量中的词元索引相对应的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87eb430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f5001",
   "metadata": {},
   "source": [
    "计算概率和目标概率损失步骤：\n",
    "\n",
    "1. logits\n",
    "2. 概率\n",
    "3. 目标概率\n",
    "4. 对数概率\n",
    "5. 平均对数概率\n",
    "6. 负平均对数概率\n",
    "\n",
    "这个过程叫交叉熵损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "639a2ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probas: tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n",
      "avg_log_probas: tensor(-10.7940)\n",
      "neg_avg_log_probas: tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(\"log_probas:\", log_probas)\n",
    "\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(\"avg_log_probas:\", avg_log_probas)\n",
    "\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(\"neg_avg_log_probas:\", neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53fa93c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "Flattened logits: torch.Size([6, 50257])\n",
      "Falttened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Falttened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "957b1db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd2c38a",
   "metadata": {},
   "source": [
    "## 5-3 计算训练集和验证集的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79cbb314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d06397",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9 # 训练集占数据集中的比例，这里是 90% 训练集，10% 验证集\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx] # 训练集\n",
    "val_data = text_data[split_idx:]   # 验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ebacfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "from data_loader import create_dataloader_v1\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb72408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    \"\"\"计算给定批次的交叉熵损失\"\"\"\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten(),\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "879186a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    \"\"\"用于计算训练集和验证集损失\"\"\"\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93f01091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583372328016\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ee260",
   "metadata": {},
   "source": [
    "## 5-4 训练大模型\n",
    "\n",
    "1、遍历训练轮次\n",
    "\n",
    "        2、在每个训练轮次中遍历批次\n",
    "\n",
    "            3、从上个批次迭代中重置损失梯度\n",
    "\n",
    "            4、计算当前批次的损失\n",
    "\n",
    "            5、反向传播以计算损失梯度\n",
    "\n",
    "            6、使用损失梯度更新模型权重\n",
    "\n",
    "            7、打印训练集和验证集的损失（可选）\n",
    "\n",
    "        8、生成本文样本养鱼可视化（可选）\n",
    "    \n",
    "回到 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d590fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_model import generate_text_simple\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                    optimizer, device, num_epochs,\n",
    "                    eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \n",
    "    # 初始化列表以跟踪损失和所见的词元\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # 开始主训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # 重置上一个批次迭代中的损失梯度\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device,\n",
    "            )\n",
    "            # 计算损失梯度\n",
    "            loss.backward()\n",
    "            # 使用损失梯度更新模型权重\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # 可选的评估步骤\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}):\"\n",
    "                    f\"Train loss {train_loss:.3f}, \"\n",
    "                    f\"Val loss {val_loss:.3f}\")\n",
    "\n",
    "        if epoch % 3 == 0:\n",
    "            generate_and_print_sample(\n",
    "                model, tokenizer, device, start_context\n",
    "            )\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    \"\"\"评估模型\"\"\"\n",
    "\n",
    "    # 在评估阶段禁用 dropout，以产出稳定且可复现的结果\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    \"\"\"生成文本并打印\"\"\"\n",
    "\n",
    "    # 进入评估模式，禁用 Dropout 层、禁用 BatchNorm 层的统计更新、禁用 梯度计算\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_length=context_size,\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "\n",
    "    # 进入训练模式，启用 Dropout 层、启用 BatchNorm 层的统计更新、启用 梯度计算\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdabbe3",
   "metadata": {},
   "source": [
    "### AdamW 优化器概述\n",
    "\n",
    "`AdamW` 是 Adam 优化器的一个改进版本，它在 Adam 的基础上加入了**权重衰减（Weight Decay）**的正确实现。\n",
    "\n",
    "#### 参数详解\n",
    "\n",
    "##### 1. **`model.parameters()`**\n",
    "```python\n",
    "model.parameters()\n",
    "```\n",
    "- **作用**：获取模型中所有需要训练的参数\n",
    "- **包含**：权重矩阵、偏置向量、嵌入层参数等\n",
    "- **自动更新**：优化器会自动更新这些参数\n",
    "\n",
    "##### 2. **`lr=0.0004` (学习率)**\n",
    "```python\n",
    "lr=0.0004  # 0.0004 = 4e-4\n",
    "```\n",
    "- **含义**：每次参数更新的步长大小\n",
    "- **选择原因**：\n",
    "  - 对于 GPT 模型，这是一个常用的学习率\n",
    "  - 不会太大（避免训练不稳定）\n",
    "  - 不会太小（避免训练过慢）\n",
    "- **经验值**：大模型通常使用 1e-4 到 1e-3 之间的学习率\n",
    "\n",
    "##### 3. **`weight_decay=0.1` (权重衰减)**\n",
    "```python\n",
    "weight_decay=0.1\n",
    "```\n",
    "- **作用**：L2 正则化，防止过拟合\n",
    "- **原理**：在损失函数中添加权重平方和的惩罚项\n",
    "- **公式**：`loss = original_loss + 0.1 * Σ(w²)`\n",
    "- **效果**：鼓励权重值接近 0，提高泛化能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abd9b3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000):Train loss 10.789, Val loss 10.765\n",
      "Ep 1 (Step 000005):Train loss 8.463, Val loss 8.634\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 2 (Step 000010):Train loss 6.925, Val loss 7.268\n",
      "Ep 2 (Step 000015):Train loss 6.272, Val loss 6.684\n",
      "Ep 3 (Step 000020):Train loss 6.095, Val loss 6.651\n",
      "Ep 3 (Step 000025):Train loss 5.970, Val loss 6.686\n",
      "Ep 4 (Step 000030):Train loss 5.928, Val loss 6.689\n",
      "Ep 4 (Step 000035):Train loss 5.874, Val loss 6.685\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 5 (Step 000040):Train loss 5.633, Val loss 6.668\n",
      "Ep 6 (Step 000045):Train loss 5.756, Val loss 6.710\n",
      "Ep 6 (Step 000050):Train loss 5.623, Val loss 6.621\n",
      "Ep 7 (Step 000055):Train loss 5.640, Val loss 6.721\n",
      "Ep 7 (Step 000060):Train loss 5.668, Val loss 6.715\n",
      "Every effort moves you, and, and, I had                                           \n",
      "Ep 8 (Step 000065):Train loss 5.511, Val loss 6.629\n",
      "Ep 8 (Step 000070):Train loss 5.122, Val loss 6.465\n",
      "Ep 9 (Step 000075):Train loss 4.697, Val loss 6.305\n",
      "Ep 9 (Step 000080):Train loss 4.316, Val loss 6.262\n",
      "Ep 10 (Step 000085):Train loss 3.871, Val loss 6.140\n",
      "Every effort moves you know the fact of the fact of the Riv to the fact of the Riv--I had been--his, and I was, and I was the picture of the fact he had been the picture. \"I had been the fact of the picture\n",
      "Ep 11 (Step 000090):Train loss 3.542, Val loss 6.116\n",
      "Ep 11 (Step 000095):Train loss 3.143, Val loss 6.093\n",
      "Ep 12 (Step 000100):Train loss 2.660, Val loss 6.072\n",
      "Ep 12 (Step 000105):Train loss 2.091, Val loss 6.125\n",
      "Ep 13 (Step 000110):Train loss 1.801, Val loss 6.094\n",
      "Ep 13 (Step 000115):Train loss 1.468, Val loss 6.135\n",
      "Every effort moves you know,\" was one of the picture.                   \"--I he had the head to me--as he's his pictures? I had the room, I can\n",
      "Ep 14 (Step 000120):Train loss 1.238, Val loss 6.245\n",
      "Ep 14 (Step 000125):Train loss 0.927, Val loss 6.200\n",
      "Ep 15 (Step 000130):Train loss 0.646, Val loss 6.254\n",
      "Ep 16 (Step 000135):Train loss 0.511, Val loss 6.341\n",
      "Ep 16 (Step 000140):Train loss 0.354, Val loss 6.437\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"       \"Oh, and my unexpected discovery; and continued to wander up and down the room, when I\n",
      "Ep 17 (Step 000145):Train loss 0.305, Val loss 6.479\n",
      "Ep 17 (Step 000150):Train loss 0.253, Val loss 6.566\n",
      "Ep 18 (Step 000155):Train loss 0.208, Val loss 6.607\n",
      "Ep 18 (Step 000160):Train loss 0.167, Val loss 6.683\n",
      "Ep 19 (Step 000165):Train loss 0.143, Val loss 6.726\n",
      "Ep 19 (Step 000170):Train loss 0.119, Val loss 6.764\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 20 (Step 000175):Train loss 0.140, Val loss 6.855\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 20\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a1e95",
   "metadata": {},
   "source": [
    "### 打印损失曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fe9128f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUFZJREFUeJzt3Qd4U+XbBvC7u7S0UHaZZe+9ZCgyBBUBQQQVkaGobDfyucCFE1FUFPyLAxBcIFNApiwZsveGFsoqlEJLZ77rfkPaFAq2UJrR+3ddh6zT5M0hyXPe+XhYLBYLRERExCl5OroAIiIicm0K1CIiIk5MgVpERMSJKVCLiIg4MQVqERERJ6ZALSIi4sQUqEVERJyYArWIiIgTU6AWERFxYgrUIi7m0KFD8PDwwKZNmxxdFBHJAQrUIg7AQHu9bcSIEY4uoog4CW9HF0AkNzp+/Hjq9WnTpuH111/H7t27U+/Lmzevg0omIs5GNWoRByhWrFjqli9fPlOLtt0uUqQIRo8ejZIlS8LPzw916tTBn3/+ec3nSk5ORt++fVGlShUcOXLE3PfHH3+gXr168Pf3R7ly5TBy5EgkJSWl/g1f75tvvkHnzp0REBCAihUrYubMmamPnz17Fj169EDhwoWRJ08e8/jEiROvWYZff/0VNWvWNPsWLFgQbdq0wcWLF1Mf52tVrVrVlIfl/PLLL9P9/dGjR9GtWzfkz58fBQoUQKdOnUwTv03v3r1x//3346OPPkJoaKh5jYEDByIxMfEGjr6Ii2H2LBFxnIkTJ1ry5cuXenv06NGW4OBgy08//WTZtWuX5aWXXrL4+PhY9uzZYx4/ePAgM95ZNm7caLl06ZKlc+fOlrp161pOnjxpHl++fLn5+++++86yf/9+y4IFCyxhYWGWESNGpL4G/75kyZKWKVOmWPbu3WsZMmSIJW/evJYzZ86YxwcOHGipU6eOZd26deb1Fi5caJk5c2aG5T927JjF29vblJv7btmyxfLFF19YYmJizOOTJk2yhIaGWn777TfLgQMHzGWBAgVM+SghIcFStWpVS9++fc3f7tixw/LII49YKleubImPjzf79OrVy7ynp59+2rJz507LrFmzLAEBAZbx48ffsv8XEWehQC3iZIG6ePHilnfeeSfdPg0bNrQMGDAgXaD++++/La1bt7Y0b97ccu7cudR9ed+7776b7u9//PFHEyxt+Pevvvpq6u0LFy6Y++bNm2dud+jQwdKnT59MlX/Dhg3mbw8dOpTh4+XLlzcnBPbeeustS5MmTVLLxqCckpKS+jgDdJ48eSzz589PDdRlypSxJCUlpe7z4IMPWrp3756pMoq4MvVRiziR8+fP49ixY2jWrFm6+3l78+bN6e57+OGHTfP44sWLTZOzDfdbuXIl3nnnnXTN45cuXUJsbKxp6qZatWqlPh4YGIjg4GCcPHnS3O7fvz8eeOAB/Pvvv2jbtq1pdm7atGmGZa5duzZat25tmr7btWtn9u/atStCQkJM8/f+/fvx+OOPo1+/fql/w2Z4Nvnbyrtv3z4EBQWle16Wl39rU716dXh5eaXeZhP41q1bM31sRVyVArWIi7r33nsxadIkrF69Gq1atUq9/8KFC6ZPukuXLlf9DfuIbXx8fNI9xn7rlJQUc/2ee+7B4cOHMXfuXCxcuNAEYvYJs4/4Sgye3GfVqlVYsGABxo4di1deeQX//PNP6knBhAkT0Lhx46v+zlbe+vXrY/LkyVc9N/vIM1NeEXemQC3iRFirLV68uKkRt2jRIvV+3m7UqFG6fVnrrVGjBjp27Ig5c+ak7s9BZBxBXqFChZsqC4Nkr169zHb77bfjxRdfzDBQ24Ima/3cOIK9TJkymD59Op577jnzfg4cOGAGp2WE5eXIdw6i4/sXkfQUqEWcDAPiG2+8gfLly5sR3xxtzcVNMqpxDh482DRr33fffZg3bx6aN29uAiVvly5d2jRBe3p6mublbdu24e23385UGfgcrOWyuTk+Ph6zZ882o7YzwprzokWLTJM3gy1vnzp1KnV/1u6HDBlimrrvvvtu83zr1683I8sZyBnAP/zwQzPS+8033zTN+azN//7773jppZfMbZHcTIFaxMkwqEVHR+P55583fcbVqlUzU6c4RSojzzzzjGkCZlM4p3Gxn5iBlUHv/fffN03GnBL1xBNPZLoMvr6+GD58uJkixf5v1qinTp2a4b6sBS9fvhxjxowxfeysTX/88cem+Zz4umwCZzDmSQj7w9mfzXITH+PfDxs2zDTXx8TEoESJEqa5XTVsEcCDI8ocXQgRERHJmBY8ERERcWIK1CIiIk5MgVpERMSJKVCLiIg4MQVqERERJ6ZALSIi4sQUqP/DF198gbCwMLP0IpdAXLt2LXKbUaNGoWHDhmYtZi5owXWf7XMn29Zl5hKTTD/IXMpcJ/rEiRPp9mEKxvbt25t5s3wezqm1T71IS5cuNStVMb0jV9b67rvv3P7/5L333jMre9nmFZOOZ9ZERETg0UcfNceL8745T5uLqthwFioXceH64HycaTj37t2b7jmioqLM4iucu810m1yfnMub2tuyZYuZU85jVapUKXzwwQdXleWXX34x89a5D8vBZVhdCRfQee2111C2bFlzrLjwzltvvWWOoY2OZw5zdFYQZzZ16lSLr6+v5dtvv7Vs377d0q9fP0v+/PktJ06csOQm7dq1Mxmetm3bZtm0aZPl3nvvtZQuXdpkXLJh+sFSpUpZFi1aZFm/fr3ltttuszRt2jT1cWY9qlGjhqVNmzYmPePcuXMthQoVsgwfPjx1H6ZAZOrC5557zqQ6HDt2rMXLy8vy559/uu3/ydq1a00Kylq1almGDh2aer+OZ+ZFRUWZzFq9e/e2/PPPP+Z9M+vWvn37Uvd57733TIayGTNmWDZv3mzp2LGjpWzZspa4uLjUfe6++25L7dq1LWvWrDGZySpUqGB5+OGHUx+Pjo62FC1a1NKjRw/zXWAaUmb4+vrrr1P3WblypTnGH3zwgTnmzFDGFKVbt261uApmbitYsKBl9uzZJlPbL7/8YlKgfvrpp6n76HjmLAXq62jUqJHJy2uTnJxsUhCOGjXKkpsx7zHP8ZYtW2ZuM8Uivzz8QtswZzD3Wb16tbnNQOLp6WmJjIxM3WfcuHEmx7At5zDzLlevXj3dazGNIU8U3PH/hPmaK1asaHI9t2jRIjVQ63hmzbBhw0yqz2th+sxixYpZPvzww9T7eIz9/PxMcCAGAR5f5t+2YcpPDw8PS0REhLn95ZdfWkJCQlKPr+21maLTplu3bpb27dune/3GjRtbnnrqKYurYPmZG9xely5dTEAlHc+cp6bva0hISMCGDRtMk44N10zmbWYrys24vCUVKFDAXPI4JSYmpjtWbKriWtO2Y8VLNlsVLVo0dR8udcklJ7dv3566j/1z2PaxPYe7/Z+waZtN11e+Zx3PrOHyqg0aNMCDDz5ougDq1q1rsnXZHDx4EJGRkeneJ9cdZzO//fFk8yyfx4b783hw7XLbPnfccYdZXtX+eLIbiOuWZ+aYuwKmM+Xa7Xv27DG3uU78ihUrUpeE1fHMeVrr+xpOnz5t+mrsfwiJt3ft2oXcimtKsy+VWZKYuYn4peWXjV/MK48VH7Ptk9GxtD12vX0YfOLi4syX113+T7huNnM9r1u37qrHdDyzhpm5xo0bZxJ8/N///Z85plwvnceQmb9sxyOj92l/rBjk7Xl7e5uTUft92G975XPYHmP+7Wsdc9tzuIKXX37ZfEZ4cshUpPyMMLe5LfuZjmfOU6CWLGEtkFmYeIYtN+bo0aMYOnSoyeFsnx9abvzkkTW3d99919xmjZqf0a+++soEasman3/+2WRqmzJlismexsxtPDlnulIdT8dQ0/c1FCpUyJxNXjnSlreLFSuG3GjQoEEmK9OSJUvSpR7k8WAz6rlz5655rHiZ0bG0PXa9fThqlCNL3eX/hM3NzIrF0disZXBbtmwZPvvsM3OdNQYdz8zjyGNmGLPHFJscFU+293K998lL/p/Y4wh6jlzOjmPuSseTswdYq37ooYdM90rPnj3x7LPPmtkfpOOZ8xSor4HNZszHy74a+zN33m7SpAlyEw46ZJCePn06Fi9efFVzFY8TUynaHyv2M/GH0naseLl169Z0X17WKBk0bD+y3Mf+OWz72J7DXf5PmL6Rx4I1FdvGGiGbFm3XdTwzj90wV04XZP8q020SP6/8Ybd/n2zaZV+p/fHkiRFPomz4WefxYN+rbR+m4+T4AfvjWblyZdNMm5lj7gpiY2NNX7I9ntDxWJCOpwM4YACby+DUFY5k/O6778woxieffNJMXbEfaZsb9O/f30zFWLp0qeX48eOpW2xsbLrpRJyytXjxYjOdqEmTJma7cjpR27ZtzRQvThEqXLhwhtOJXnzxRTPK+YsvvshwOpE7/p/Yj/omHc+sTXHz9vY204r27t1rmTx5snnfkyZNSjediO/rjz/+sGzZssXSqVOnDKcT1a1b10zxWrFihRmRbz+diCObOZ2oZ8+eZjoRjx1f58rpRCzLRx99ZI75G2+84XLTiXr16mUpUaJE6vSs33//3Uz94ywCGx3PnKVA/R8495Q/mJxryqksnBOY2/B8LqONc6tt+AUdMGCAmW7BL1vnzp1NMLd36NAhyz333GPmSvKL//zzz1sSExPT7bNkyRJLnTp1zPEuV65cutdw5/+TKwO1jmfWzJo1y5y48KSjSpUqlvHjx6d7nFOKXnvtNRMYuE/r1q0tu3fvTrfPmTNnTCDhnGFOc+vTp4+ZQmePc4Y5FYzPwWDGgHWln3/+2VKpUiVzPDk9bs6cORZXcv78efNZ5GfC39/ffG5eeeWVdNOodDxzlgf/cURNXkRERP6b+qhFREScmAK1iIiIE1OgFhERcWIK1CIiIk5MgVpERMSJKVCLiIg4MQXqTIiPj8eIESPMpdw8Hc/speOZvXQ8s5eO583TPOpM4PJ4TOPG9I5colFujo5n9tLxzF46ntlLx/PmqUYtIiLixBSoRUREnJjb56NmarWNGzea1IFXZoTJrJiYGHMZERFhmnHk5uh4Zi8dz+yl45m9cuPxTElJMek4mRudqWtvltv3Ua9btw6NGjVydDFERCSXWbt2LRo2bHjTz+P2NWrWpG0HjAnmRUREbqXjx4+bCqIt/twstw/UtuZuBumSJUs6ujgiIpJLeN5gd+tVz5MtzyIiIiK3hAK1iIiIE1OgFhERcWJu30ctImKTnJyMxMRERxdDXJyPjw+8vLxy7PUUqDMpPikZaw9GwcvTA03LF3J0cUQkCzgLNTIyEufOnXN0UcRN5M+fH8WKFYOHh8ctfy0F6kz66Z8jGDFrB1qF+aNp+daOLo6IZIEtSBcpUgQBAQE58uMq7nvSFxsbi5MnT5rbOTHtV4E6k1qXSEYd31dR+vhJnI/dh+CAPI4ukohksrnbFqQLFizo6OKIG8iTx/r7z2DNz9WtbgbXYLJMKlW6LMI8T6OAxwXsXP2no4sjIplk65NmTVoku9g+Tzkx5kGBOrM8vXCwUAtzNWn7H44ujYhkkZq7xVU/TwrUWeBXo5O5rBC1DCnJyY4ujoiI5AIK1FlQ4bb2uGjxR1FE4cDmFY4ujohIloWFhWHMmDGZ3n/p0qWm9nirR8x/9913ZiS1XE2BOgt8/QOwI+g2c/3cv785ujgi4sYYHK+3jRgx4oYzCj755JOZ3r9p06YmyUS+fPlu6PXk5mnUdxYlVGgPbFqK0ON/cZw+v02OLpKIuCEGR5tp06bh9ddfx+7du1Pvy5s3b7opQxzdnpncx4ULF85SOXx9fc18YXEc1aizqGLzLoi3eKNEcgTOHd7q6OKIiJticLRtrM2yFm27vWvXLgQFBWHevHmoX78+/Pz8sGLFCuzfvx+dOnUy6RUZyJkL+a+//rpu0zef95tvvkHnzp3NSOaKFSti5syZ12z6tjVRz58/H1WrVjWvc/fdd6c7sUhKSsKQIUPMfpwSN2zYMPTq1Qv3339/lo7BuHHjUL58eXOyULlyZfz444/pTk7YqlC6dGnz/osXL25e0+bLL78078Xf398cj65du8JVOTRQL1++HB06dDAHmB+EGTNmpHuc/xE8i+SEcs5ba9OmDfbu3QtHKlKoEDb71jXXI9b87NCyiMhNLFqRkOSQja+dXV5++WW899572LlzJ2rVqoULFy7g3nvvxaJFi7Bx40YTQPkbe+TIkes+z8iRI9GtWzds2bLF/H2PHj0QFRV1zf254MdHH31kAid/x/n8L7zwQurj77//PiZPnoyJEydi5cqVOH/+/FW/7/9l+vTpGDp0KJ5//nls27YNTz31FPr06YMlS5aYx3/77Td88skn+Prrr01c4PPXrFnTPLZ+/XoTtN98803TCvHnn3/ijjvugKtyaNP3xYsXUbt2bfTt2xddunS56vEPPvgAn332Gb7//nuULVsWr732Gtq1a4cdO3aYsyRHiSrVFjiwDkEHOZ/6bYeVQ0RuTFxiMqq9Pt8hr73jzXYI8M2en14Gorvuuiv1doECBcxvqs1bb71lAh5ryIMGDbrm8/Tu3RsPP/ywuf7uu++a3921a9eaQJ8Rzh3+6quvTG2X+Nwsi83YsWMxfPhwU0unzz//HHPnzs3Se+OJAMs1YMAAc/u5557DmjVrzP0tW7Y0JwdsXWAFjmtvs2bdqFEjsy8fCwwMxH333WdaHsqUKYO6da0VLFfk0Br1Pffcg7fffjv1P9MezzrZPPPqq6+aphyeLf7www84duxYls/Mslto4y5ItnigdPxeJJ055NCyiEju1aBBg3S3WaNmzZZN0mx2ZrM0a9v/VaPm76sNA1xwcHDqEpkZYRO5LUgTWz1t+0dHR+PEiROpQZO4cheb6LOC5W7WrFm6+3ib99ODDz6IuLg4lCtXDv369TMnJGxyJ568MDjzsZ49e5raPVsBXJXTDiY7ePCgWZ+XZ0s27Kdp3LgxVq9ejYceeijDv4uPjzebTUxMTLaXrUbF8tjoURUNsAMRa35FmfZpTT4i4vzy+HiZmq2jXju7MKjaY5BeuHChqXVWqFDBdBmybzYhIeG6z8MaqT12RaakpGRp/+xs0s+MUqVKmWZt9sHzPbPm/eGHH2LZsmWmFv3vv/+a/vUFCxaYLlT2Z3PEuytOAXPawWQM0sRBAPZ42/ZYRkaNGmUCum2rVq1atpeNGbSOFGmFKEte7I88m+3PLyK3FgMLm58dsd3KFa3YH8zmYrZSsr+WTcOHDuVsqx9/d/k7zaBowxHpDJxZwVYBvh97vG3/m84TEfbBs6meQZmVuK1brYN8OQKeFT12obLvncdh8eLFcEVOW6O+UewXYV+GTURExC0J1j6N+6LhL41RMSY/WmX7s4uIZB1HOf/+++8mePGEgON6rlczvlUGDx5sKk2s1VepUsX0WZ89ezZLJykvvviiGeBWt25dE3BnzZpl3pttFDtHn/MEgK2sbIqfNGmSCdxs8p49ezYOHDhgBpCFhISY/nEeB44cd0VOW6O2zdtjX4c93r7enD4O02f/im1jE8it0LxKKVg8vLArMgYR5+JuyWuIiGTF6NGjTWDiIiUM1hx8W69evRwvB6djcXDaY489hiZNmpi+cpYlK4OAOZXr008/Nc341atXN6O7OYr8zjvvNI+zCXvChAmm35p97AzgDOacDsbHGNRbtWplauYc+PbTTz+Z53FFHpac7li4Bp5pcTCAbZ4di8VpW+xz4fB84hB/phTjmdS1+qivFB4ebvoyjh49ipIlS2ZrmR8YtwobDkfhs7ZB6NjKmrBDRJzLpUuXzJgXzhxx5GyR3Iy1WQZM1pA5Et3dP1fh2Rx3HNr0zRGK+/btS73NN71p0yYzxYBD7Z955hkzKpzNObbpWQzeWZ00f6vcW94HYyMHo/DyGKDpfsBfS+yJiBw+fNgM4mrRooUZ3MvpWfx9f+SRRxxdNJfk0EDNSemcD2dj61vmCjasNb/00ktmrjXXpeWqOM2bNzcT153lrLhJjcqIW+GHBFxEyrFt8CuXfiqBiEhu5OnpaX7D2SLK1tEaNWqYpmnWqsXFAjX7Gq7X8s7mcE6it59I70yqhgahu9//YXNMEMYnVYIav0VErFOnrhyxLW44mMwV8ESifNU6iIcvluy69uIAIiIiN0qB+ibdWbmIuVy86yQsiRr9LSIi2UuB+iY1r1AIbb03YuKF/oj5baijiyMiIm5GgfomBfp5o0yxIijveRy+++cDyda1ZkVERLKDAnU2CK3V0iwn6p94DjiyytHFERERN6JAnQ1aViuOv5KtmWEStv3h6OKIiIgbUaDOBmULBWJjYHNzPWXHLC7D4+giiYikToPl4lE2YWFhJoXwf81oyY50wtn1PNfDrFh16tSBO1OgziaB1drggsUf/nEngGMbHV0cEXFxXKv77rvvzvCxv//+2wRBZoXKKma14iJSOREsjx8/jnvuuSdbXys3UqDOJndULYWlKbXNdcvOWY4ujoi4uMcff9zkWea60VdicooGDRqYZBRZVbhwYZNtKicwgRITJcnNUaDOJo3LFcASj8Zp/dTOketERFzUfffdZ4Iql+K8MkfCL7/8YgL5mTNnTJaqEiVKmODLHNTMEnU9VzZ9792716SD5NLMTAnMk4OMsmFVqlTJvEa5cuVM3oXExETzGMs3cuRIbN682dTyudnKfGXTN3NFM6MV01EyyxVr9nw/NsylzVwOzJgVGhpq9hk4cGDqa2U2AQhXs2QyDJ4ksKbPpadtEhISMGjQIPP8fM9Mi8mUnMSVMtk6wFwT/FvmlhgyZAgcze3yUTuKn7cX4su2QfyhL+EXfQA4tRsoUsXRxRKR60m4mPW/8fIDvC7/dHI6ZnI84OEJ+OT57+f1Dcz0y3h7e5s0kQx6r7zySmouZwZp5mFmgGaQq1+/vgmkTOs7Z84c9OzZE+XLl0ejRo0yFdS6dOmCokWL4p9//kF0dHS6/mwbpgtmORi4GGz79etn7mM+hu7du2Pbtm0mGNpyRefLd3WCIuZtYKpLpr1k8/vJkyfxxBNPmKBpfzKyZMkSE0R5yaRN3bt3N8GWr5kZTI358ccfm7SYzGX97bffomPHjti+fbtJ8PTZZ59h5syZ+Pnnn01AZoYrbvTbb7/hk08+wdSpU01KzMjISHMC4mgK1NmoabWyWHmgBlp5bQJ2zVKgFnF27xbP+t88+B1QvbP1Or/nv/QGyjQH+sxJ22dMTSD2zNV/OyI6Sy/Vt29ffPjhh1i2bFlqHmY2ez/wwAMmGHJj4gubwYMHY/78+SYIZSZQM7Du2rXL/A2DML377rtX9Su/+uqr6WrkfE0GMwZq1o6Zb5onFmzqvpYpU6aY1JA//PADAgOtJyzMqsW++Pfff9+cLBDzafN+Ly8vVKlSBe3bt8eiRYsyHahZG+eJiy0VMp+bQZ+tCF988QWOHDliAjaTPPHkhzVqGz7G99CmTRv4+PiYQJ6Z43irqek7G7WsUhjzUxqa60nbZzq6OCLi4hiomjZtamqFxBomB5Kx2ZtYs2Z+ZzZ5Mz0wAyaDLgNOZuzcudMk0LAFaWKN90rTpk1Ds2bNTBDjazBwZ/Y17F+rdu3aqUGa+Jys1e/evTv1PtZkGaRtQkNDTe07M86fP49jx46Z57XH23x9W/M60ylXrlzZNGszHafNgw8+iLi4ONO8zxOD6dOnIynJ8YtYqUadjULz5cGhgncgOfobeJ/YApw7AuQv7ehiici1/N+xG2v6tqnSwfocbPq298xWZBcGZdaUWRtkbZrN2szzTKxts6mXtUUGawZBNl2zHza7rF69Gj169DD90Gy6Zi2etWk2L98KrMna8/DwMME8u9SrV8/kxp43b55pUejWrZupQf/666/mpIUnDbyfffUDBgxIbdG4slw5STXqbFa/WiWst1RGlHdRa6AWEefFPuOsbrb+aeJ13mffP329570BDCTM78ymYzYbsznc1l/NVJKdOnXCo48+amqrrAnu2bMn08/N/NDsn+U0Kps1a9ak22fVqlWmeZj95Bxpzmbjw4cPp3+7vr6mdv9fr8X+XvZV27D8fG+s3WaH4OBg0zpwZYpN3uZAOfv92Pc9YcIE01rAvumoqCjzGJvy2RzPvuylS5eaExX2yzuSatTZrGWVIui39FlYPEPwb+lmSGvAERHJOjY1M6gMHz7cNO2y6daGQZM1QQZT9u2OHj0aJ06cSBeUroc1SY7m7tWrl6k58vkZkO3xNdjMzVp0w4YNzYA1NgnbY781a6lsUuZoaw40u3JaFmvlb7zxhnktjqw+deqUaSng4Ddb/3R2ePHFF83rsOWBg9DYCsFyTZ482TzOY8TmdA4040kCB+exST9//vxmUBtPOBo3bmxGuE+aNMkEbvt+bEdQjTqb1S2VH5Y8BRB9KQkbj5x1dHFExA2w+fvs2bOm6dm+P5l9xWzK5f0cbMaAw+lNmcVAxaDLflkOmuIo7HfeeSfdPhwx/eyzz5rR2Qx8PCng9Cx7HNzGxVlatmxpppRlNEWMgY/956y5MuB37doVrVu3NgPHstOQIUPw3HPP4fnnnzfdARyNzlHePOEgnkR88MEHpnWA5Th06BDmzp1rjgWDNWvZ7NPmHHU2gc+aNctME3MkDwsnjrkxLhbAfgc27/BMLycM+WkjZm4+hieblsD/ldwK1OoGeGvSv4gjcKQxa3tly5Y182ZFbvXnKrvjjmrUt8D9da1nvO3/fQKYOQjYZG1yERERySoF6lvgzkpFUK5QIKYnNsFFv8KAT84s1yciIu5HgfoW8PT0QJ9mYZiS3Br3e32B5JrdHV0kERFxUU4dqDn6joMW2AfAkXccxcfJ/a7Qrf5A/ZLIkycAe6OSsHhX5ibri4iIuNT0LC79Nm7cOHz//fdmtZr169ejT58+ZsK9MyyUfj0Bvt54uFFpfLVsP779ey/uSv4b8PYHqt7n6KKJiIgLcepAzWkAnMzPtV5tc/U47H/t2rVwBY81KYMJfx9A2JHfgeP/A/KXASq1A7wct8KNSG6VnatbiaTk4OfJqQM117gdP368WWmHk/K5qs2KFSvMhPVriY+PN5tNTEwMHKV4/jy4t2YoZmxuhv/z/w1B5w4DW6YBdR91WJlEchuumsU5slwDmnN8edu2spdIVrHrlUu0csEWfq74ecrVgfrll182K+VwYXou0s4+a07G5wo318K8olyT1lk83rwsZm0+hi8T7sUwrynA8o+AWg+lX4ZQRG4Z/phynAuXyWSwFskOXMCF2bX4+brVnDpaMFUbl33jGrfso+YycFxwnivzcBm6jHCZPa5KYxMREZHp5fRuhTql8qN+mRB8d7gNBvvNRcDZg8DWX4A6DzusTCK5DWs9/FFlJqT/WpNa5L+w4si0njnVMuPUgZprtrJWbcsryuXguBg8a83XCtRcX9Z+jVnWyB2tb7OyGHj4LCYkt8dQTAaWfwjUfFC1apEcxB9VZkByZBYkEbebnhUbG3tVswLPZFxtUEi76kVRIn8efB3XGvE++YGo/cC23xxdLBERcQFOHaiZaox90szWwoXTuXg8B5J17twZrsTbyxO9m4YhFv6Y7NnBeidr1SlqghMRERcO1GPHjjUZVpi8m7lMX3jhBTz11FNm0RNX061hKQT4euHj6DuR6JsfOLMX2Pa7o4slIiJOzqkDNdORjRkzxvRLMw3b/v378fbbb+fIcPjsli+PD7o1KIWLyIOZAZfT0C3/QLVqERFx3UDtbrj+NwcJjohsjmS/fMDpPcD29AnYRURE7ClQ56AyBQPRpmpRxCAAS/J3teurdq3BcSIiknMUqB0wVYuGH2uKlMCiQLmWQFKco4slIiJOSoE6h91WrgCqhQbjVGIejK8/A7jnPcA30NHFEhERJ6VA7YBFF7isKH235jgSky83e0cdBJITHVs4ERFxOgrUDnBf7VAUyuuHyPOXMHfrcSA6Aph4DzDpASDunKOLJyIiTkSB2gH8vL1MCkz634qDsHClsvgY4MIJVrkdXTwREXEiCtQO0qNxafh6e2JLeDQ2eNQA+swDHvkZ8M/n6KKJiIgTUaB2kIJ5/dClbonUWjVCawEh1lq28c94YOcsxxVQREScggK1A/W5PFVr/vZIbIuITnvg8Gpg3kvAtJ7AqrHMVO64QoqIiEMpUDtQ5WJBaF2lCFIsQI9v/sHmo5cHkpVsCDR8HIAFWPAqMOd5IDnJ0cUVEXEPiZeAU7uB2Ki0+/YsAMLXwxkpUDvY6O51UK90fkTHJZpgvf5QlDVP9b0fAW3f4YQuYP3/gJ8esg44ExGRq7EyE3cWOHsYiNxmbZlk8N3yM7Dq8/T7Tu4KfNEI2Lsg7T6mVD6xHc7I29EFyO2YrOOHxxvj8e/W4Z+DUej5v7X4X68GaFqhENB0kLXf+rd+wL6FwLd3A91+APKXsQbzm8WEIAkXgIRYwJIMWFKszey8ZG3eOw8QHJq2P89A+ViB8oD35cQoPCO9FG0dre7hmX7z9Aa8fAAvP8DL1/pFyCw+7/kI63Pbb/EXgLxFgJAw6xZcIuvHgvPVzx0BzuxL2/h6qeX2snsfHkDhqtb/C5u1EwCfPEDle4GAAll7bRHJWFICEBcFBBVLu2//Euv3M+x2oEgV631H1wFL3gYunbdWXuIvXybGXv/5G/QFfAOs1/nbcWxj+spPifpA/jA4Iw+Lxb07QMPDw1GqVCkcPXoUJUuWhLOKS0jGkz+ux997T8PP2xNf96yPOysXsT4YsQGY8hBw8WTaH3j7W1c0881rDRhc4czmj0HWQNJuVFoQW/o+sHuONdAxOGfmg13hLuDRX9NuvxNq/Zshm4AC1v51LHwdWPlp5t6kCdy+QGgdoO+8tPs5f/z0XuDR34BCFS+X9z1g6ajMPWe+ktYvXrGaQNu30x47f8x6clH2DmvwpT+HA/98bT0xySwu8/rYDOt1fl3eK239cRi4Fihc2Xr/um+A7TOA/KUvb2WsJ1ksk19Q5l9LxB3x5DjmuPU7GR1uPQnn+hG8tF3n7xt/H147lfZ3Pz0M7J4L3DcGaNDHet+BpcAPna79Wqxg8Dtn2/yDgXylrL8NthPrpHjra92i6bDZHXdUo3YSeXy9MOGxBhg05V/8tfMk+v2wHp8/Ug/tqheznuk98Rfwax9r0KakS9Yt9oz1LNSGH8CNP1qvt3wFyJPfep1fhuObM35xDy9rwLPVitnczus+/un3CygEJF607mvDDztPFkxtPIPNXkqSdUuOT3//yV3A+XDre8HlQB1YCMhb1DpdzX7jCUhMJHD2kLVWnJxgvc6NZ9j2vu9gPRsfutkayIlfXAZpnwBry0DBckDBCtbXsrUmpLYuXN4YdO1/cGp0sb42v/w2xzYBh/7O4Nh6AqG1gTLNgDJNgdJNVAsX18XvCL+nDLicqWLDLIBsNmaloUQ9632HVwFzXrD+PnGNiCt/D67Vysf+Y9tvT6nGl0/G7b5rRaoDncdbfw/sg7FfsPW3yNbadz3efnAlqlE7mYSkFDwzbSPmbo2El6cHxnSvgw61i6dvHkqw1YovX/IDa6vZMVCv/sJ6f7NnrB9gOr7FGuD88lo/zLYPuPlg+92aM0tmBUtJtAZTlpuXDNI8Mchv98U7ssZ6H5u2slL75PPzLN0WqBnEGURtj71fxtqM1mUCULyO9f4LJ60nC0Gh2fueT+yw/lCdu3wCYZrW9wPRR6/elz80DNpV2gPlW2ZfGUSyA7uY2M9rPseHL1+/fJvXebLO7+vrZ9K+Q1N7ALtmA+0/Bho+Yb3v4HLrybKNp4+1Ky24JJCvhLXbiq1hwcXTrgcUdItFn8KzOe4oUDuhpOQUvPjrFkzfGAFPD+CDrrXRtb5rlF2uwGY+Dmo5vMJaw2AOcpvbBgB3X27eT7gI7JgJlL7NWvt3gx8rcYbabxQQWDDtvv2LrS1YYc3TasTsq53/KnDpnPXzysv/krcYMHST9eSYNnxnHcBV/X7rcxMHdkX8C+QJsQbiwMJZG6fiwsLV9O3+vL088fGDtU1f9dR1R/HCL5sRn5SMHo3tmmDFNbCWUOtB62ar0R9ZDRxaCVS+J20/TguZ8bS1tvGc3chT1mD4HLY+dhF7bDm6EAlEHUjb2JLDJD+8ztajVyLTAuS/PwLbfwfufj8tULMVjieSV3ZzcawFx1mw6yf1epj183hlt1j93leXjQG6Qutb9c5zFQVqJ+Xp6YF3O9eEv48Xvlt1CK9M34ZLiSmpmbfERXHEerVO1s0e++84f5795fY/wuNbWPvtSjWy9m9zY583BxKq1p27nD9uXa3QNEcfuhyYD14/nz3HSDCQs3mZ+PnhfbbBoFSoEtD1W2sXWlBxa1BmF5k4DTV9Ozn+97z35y58veyAud3v9rLoXLckqhQLMsFc3Ay/jrYAfO4o8GUTICGD+fPs77tyoJ1ta/5s2g/xxdNAYpy1FqTA7nw4OJHN0xygxYBpm6Wx5itg229AvceAej2t94VvAL5pdfVzmDEfpYECHBhZ3npp23i/iw2ccgfhua3pOyIiAsOGDcO8efMQGxuLChUqYOLEiWjQoAFyS/7ql++uAn9vL3y6aC8m/H3QbAUCfdGkXEE0rVAQzcoXQpmCAWZfcXH2/4cccDfsEHByu7Wf+8jljSNoOUgv9rR1u1KjfmnXN/9kXd2uVnegy/i0kwH2HXLwHmvmkjEGUI4pOLXLOs2Pl5xGOPCftOO27ENr/yxXErz9Oet9TFW7/ENr0y9PnHiiZP6vzgAXz1gvbbc5cMvmmW1pgyw5SyN8rXXGhw1Pvqrclzb1j7MWeB+DMdcrELfl1IH67NmzaNasGVq2bGkCdeHChbF3716EhIQgN2EAfvauSihXOBC//xuBtQejEHUxAXO2HjcbFc/nbxZJaVq+IJpVKISiwVf0IYlrYg2LTd3cbnvaGmQ5op9T0a5cDIaDgHjJgTv2wYbTW+yb1Nl0ampmHtZaV9HqQNEa1mljHBzEOfoZXfI5bAGBzfG2BWFyigl4UdbpPmxRMOsIXN5uZE4sZyLwJMgsnHHeOisiNSjvTr9ugT0G7+J1rdfN3OBw61RJG45DWH3FSljX5WE99vx/tanZ1RqkOQ/fhvs8NDlr71Fyb9M3q/MMHrYq/dq1azFlyhRUq1YNTz75ZLYV7uWXX8bKlSvx998ZzE/NJU3f15rCtTn8HFbuO41V+89g45GzSExO/99YvnAg6pUOQdXQYLNVCw1GvgCddedKZmpcfNrUtyP/ANMevXYgupbnd6etGjX3JWDteKDFMKDl8LSBb1ygggONuOiEueTmd/UlV6uz3WZt3zYyec9868hkjhyuenlqz+l9wNd3WKcFXQtPRhiwfeyC9/1fWk9CaPWXwKrPgFrdgLvetN4XcwL4uNL13zPn73LqY+Eq1qZpXnIQlm20M4M7a78c0cyaLXGO8ZovraOeeeLEOfucdmTbuEZA6u1C1rUONFjQrYQ7Q9P3I488YgJyz549ERkZibvuugvVq1fH5MmTze3XX38d2WHmzJlo164dHnzwQSxbtgwlSpTAgAED0K+fXdNeLsQ81g3DCpjtmTZAbEIS1h86i5X7T2P1/jPYGhGN/acums1eifx5LgftIOtl8WCUCglQX7e74wIQ9otAlG4MvLjXWvPj3G9up3Zaa5asGbLmai65xabdx6BqYwYwWdIvfsNaKWuoWcWRwbZAzTn1/3xlvW4L1FwLwBak+XpsUjbL315MWzyHo5ttLQs29gtssKuAtV++Zxvb4Ck+P68zaHJlPAZjBmcG5v8aVMUTF/slL4kDt+xXyBNxRI2aTc9r1qxB5cqV8dlnn2HatGmm5rtgwQI8/fTTOHDAOvDpZvn7W38YnnvuOROs161bh6FDh+Krr75Cr169Mvyb+Ph4s9n3cbOm70416v8SHZuIfw6ewbZj57HzuHULP5vxyNBAXy+TxatQXj8E5/FBkL83gv19zPVgXjeXvJ3+fvWH53IMklyGljVLBjnibU4zSxfs4y4veMOV9OIvX15xm3PJORqeWJs++Ld1dHKltmmj37mQDGugXH3K/rPHRAwM4gkZbGWapLUicMQ0+/YzCqwi7rjgSd68ebFt2zaEhYWhY8eOph+ZA76OHDligndc3HWmC2SBr6+vGTS2atWq1PuGDBliAvbq1asz/JsRI0Zg5MiRV92fmwJ1Rpida9floL3DXMZg94kY04yeVd6eHggJ9EXBQF+EBPiiQN606wXzXr4M9EXZwoEIzXe5iVBEJJcId4ambzZzs1bbvn17LFy4EG+99Za5/9ixYyhY0G4VnJsUGhpqasP2qlatit9+++2afzN8+HBTA7+yRp3bMUtX43IFzWa/AtqB0xex98QFnItLwPm4JJy/lIjzcYk4fykJMXbXrZeJZi53UooFp2LizfZfGoaFoFOdEmhfM9QEdxERyYFA/f7776Nz58748MMPTRN07dq1U/uUGzVqhOzCmvru3bvT3bdnzx6UKXPtFbr8/PzMZnP+/BWJGiTdCmiVigaZLbMuJSbjbGwCzlxIMJccfW67fuZiAqIuJCDKPB5vTgLWHTprtpGztqNFpcImaLepWtQkIckKnlQcPH3RbCVDAlCxaF74eOWO5QhFJHe7oUB955134vTp0yYI2k+V4gCzgIDL+T6zwbPPPoumTZvi3XffRbdu3czo8vHjx5tNHIMrpbE5OzNN2sej4zBr8zHM2HjMNLczKxg39ou3q1EM99cpYaaT8YTB3tmLCdgZaW2eN831keex58SFdM30HFDHRV9qlMiHGsXzoWaJfKhULC/8vDV6VkTcyw31UbMPmn9mC8qHDx/G9OnTTbM0R2lnp9mzZ5vmbM6fLlu2rGnWzsqob3ecnuWK9p6IwYxNEfhj07F0A9s4iK1D7VBzAmDtQ49B5Hm7Oal2GODDCgXiSFQsYi4lXfW4j5eHaR1g4K5RIhgVigSZz2lCcoqZvsZAn5icYi6t91mv8zI+KQWxCclmi0tIQlyi7Xqyuc5LczsxGcWC/VE/LMQ06zcoUwAlQ/JocJ2IONdgsrZt26JLly5mhPe5c+dQpUoV+Pj4mFr26NGj0b9/fzgLBWrnwo/bhsNnTdCes+U4zsYmZrhf6QIBqBoahCrF0uaBMyByKhmfg8F6W8R5MxVt+7Foc3nuGs91qxUJ8jNT5eqXCUGDsBBT1itbCUQk9wh3hkBdqFAhM6+Zg8q++eYbjB07Fhs3bjSDvDiHeufOnXAWCtTOi7XZv/eewvztkSaw2eZ4s1Yc5J+1xVn4MY44F4dtEdEmgG87Fo0jZ2Lh7eVh+rK5sbnc9/Ila9/29zFTWR5fb+Tx8UKAr5fpQ7dd9/f1QoC57g0/H08cOHUR6w9FYf3hs+b1OLjOHv+ubun8aBBWAF3rlUTpgtnXHSQizs8pAjWbvHft2oXSpUubvmMG7DfeeMMUitOzuCa3s1CglluJTeJcJY6tBAzevOQoeRueBDx+e1kMbFkBef2cesVeEXGn6VlMjDFjxgwz8nv+/Plm0BedPHkSwcHBN10oEVfBmvdt5QqajVJSLNh78gLWH44yTftc4nXc0v34dUM4XmxX2dSwtRKciGTFDXWksXn7hRdeMAuecDpWkyZNzP1cmaxu3cuL1YvkQgzCXOmtR+MymPxEY3zzWAOULRRo5py/9OsWdPpiJdYdinJ0MUUkN+Sj5prex48fN3OoPT2t8Z7Tp1ij5uAyZ6Gmb3GGvvjvVx3CZ4v2Iibe2ix+X61QDL+3qll/XUTcS7gz9FFfWSBy1iCoQC3O4vSFeHy8YA+mrjtislVyANtTd5TD03eWNwPVRMQ9hGdz3Lmhpu+UlBS8+eabyJcvn1kljFv+/PnNUqJ8TESuxjnjo7rUxOzBzdG4bAEzd/uzxfvQ6qNlmLExwoxcFxHJlkD9yiuv4PPPP8d7771npmVx4+phnKb12muv3chTiuQa1Yvnw9Qnb8O4HvXM3HAu8PLMtE14aPwa7D91wdHFExEnc0NN38WLFzdJOZg5y94ff/xh8kUzEYazUNO3ODOunf6/FQfx+eJ9ZtUzTuca0LI8+t9ZXsuhiriocGdo+o6KispwwBjv42MikjlcOpVzrBc8ewfurFzYLG065q+9uOfTv7HmwBlHF09EnMANBWqO9GbT95V4X61atbKjXCK5SqkCAZjYuyE+f6QuCgf5mdXP2BT+0q+bTZISEcm9bmio6QcffGByUf/111+pc6hXr15tqvlz587N7jKK5ApM7HFfreK4vWJhfPDnLkz+5wh+Xh9uMo69dl9Vk21MyT9Ecp8bqlG3aNHC5IXmymRMysGNSTq2b9+OH3/8MftLKZKL5Mvjg3c618Rv/ZugUtG8Juf3s9M2o+f/1uLQ6YuOLp6I5LCbnkdtb/PmzahXrx6Sk5PhLDSYTFx9sZQJfx8wi6VwOheTiAxuWQH97ihn+rdFxPk4xWAyEckZDMy2wWa3VyxkAvfHC/eg9cfLMHPzMc29FskFFKhFXECZgoH4oW8jfPpQHYTm8zcpPYf8tBFdxq0yGbtExH0pUIu4CA4k61SnBBY/fyeev6uSyZW98cg5PDBuFQZN+RdHo5wnvayIOGjUNweMXQ8HlYnIrU+tObh1RXRvWMqsHf7zhqOYveU4Fuw4gb7NypoFU4L9fRxdTBFxRKDm2t7/9fhjjz12s2USkUwoEuyP97vWQq+mYXhn7g6s3HcGXy3bj1/WH8Wzd1XCQw1LwdtLjWYiri5bR307I436ltyAX+Mlu0/i7Tk7zWIpVLFIXrzeoZqZly0iOUejvkUkw/7rVlWKYv4zd2Bkx+oICfDB3pMXzNzrF37ZjOjYREcXUURyQ6Bmti7+ID3zzDOOLoqIU/Lx8jRN4UtfbIneTcPAhcx+3RCONp8sw4LtkY4unoi4c6Bet24dvv76a60lLpLJ1c1GdKyOX59ugnKFA3EqJh5P/rgBg3/aiDMX4h1dPBFxt0B94cIF9OjRAxMmTEBISIijiyPiMuqXKYC5Q243aTO9PD0wa/Mx3PXJcnPp5sNTRNyGSwTqgQMHmiQgbdq0cXRRRFwOlxoddncVzBjQDFWKBZm1w1mzfurHDTh5/pKjiycityJ7Vk6aOnUq/v33X9P0nRnx8fFms4mJibmFpRNxHTVL5sPMQc0xbul+fL5kr5l3zZzXr3eojgfqKTOXiLNy6ho1h7YPHToUkydPhr+/f6b+ZtSoUWY+t22rVq3aLS+niCutHT60TUXMGtwctUrmw/lLSWZUeO+J63D4jDJziTgjp55HPWPGDJNK08srLUsQM3PxzN/T09PUnO0fy6hGHRERYYK15lGLpJeUnIJvVhzE6IV7TLIP9mF3qlMcA+6sgApF8jq6eCIuKzyb51E7daBms/Xhw4fT3denTx9UqVIFw4YNQ40aNf7zObTgicj17T91AW/O2oFle06Z22wBb18zFINaVUCVYsGOLp6Iy8nuuOPUfdRBQUFXBePAwEAULFgwU0FaRP5b+cJ58X3fRtgSfg5jF+/Dwh0nzNrh3NpWK4rBrSqa/m0RcQyn7qMWkZxTq2R+THisAeYNvR3ta4WamjUHnHX4fAV6T1yLDYejHF1EkVzJqZu+s4OavkVuzL6TMfhyyX78sfkYklOsPxNNyxc0TeJNyxdydPFEnJbW+haRHFGhSBBGd6+Dxc+3MJm4fLw8sGr/GTwy4R/0mbgWB09rlLhITlCgFpHrKlMwEO89UMusH/5YkzImYC/ZfQptP1mG9+btwsX4JEcXUcStKVCLSKaUyJ8Hb3aqgT+fuQMtKhVGYrLF5L9u9fFS/LEpQkuSitwiCtQikuVR4t/1aYhvHmuA0gUCcOJ8PIZO3YTuX6/BjmPnHV08EbejQC0iWcZFh9pUK4oFz96BF9pWQh4fL6w9FIX7xv6NV2dsxdmLCY4uoojbUKAWkZtK+DGoVUUser4F7qsVCg4On7TmCFp+vBST1hxOHS0uIjdOgVpEblrx/Hnw+SP18FO/21C5aBDOxSbi1Rnb0PHzFdgdqcQ4IjdDgVpEsk2T8gUxZ0hzjOhQDcH+3th+7LxZMOX7VYc02EzkBilQi0i28vbyRO9mZbHo+TtxZ+XCJuHHGzO3o+9363AqJi1hjohkjgK1iNwShYP8MLF3Q1O7ZnpNzr2+59PlWLLrpKOLJuJSFKhF5JaODmftetag5qbv+vSFBPT5bh1GzNyOS4nJji6eiEtQoBaRW65ysSD8MagZejcNM7e/W3UInT5fiV2Rmnct8l8UqEUkx6ZyjehYHRP7NEShvH7YfSIGHT9fiW9XHNRAM5HrUKAWkRzVsnIR/PnM7WhVpYgZaPbm7B3oPVEDzUSuRYFaRHIca9T/69UAb3aqDj9vTyzbcwr3fvY3Nh095+iiiTgdBWoRcdhAs8eahGHW4OaoVDSvqVF3/3o1Zm0+5uiiiTgVBWoRcahKRYPw+4BmaF2lCOKTUjD4p40Y89ce9VuLXKZALSIOl9fPG+Mfa4B+t5c1t8f8tRdDpm7SFC4RBWoRcRZenh54pX01vP9ATXh7epgm8IfGr8HJmEuOLpqIQylQi4hT6d6wNH58vDHyB/iYwWX3f75Sea4lV1OgFhGnTO4xY0AzlCsciGPRl9D1q1VYuOOEo4sl4hBOHahHjRqFhg0bIigoCEWKFMH999+P3bt3O7pYIpIDwgoFYnr/ZmheoRBiE5Lx5I/r8dWy/RpkJrmOUwfqZcuWYeDAgVizZg0WLlyIxMREtG3bFhcvXnR00UQkB+QL8DErmfW8rQwYn9+btwsv/brFLJQiklt4WFzo9PTUqVOmZs0Afscdd2Tqb8LDw1GqVCkcPXoUJUuWvOVlFJFbgzmtR87ajhQLUDIkD3o1CUO3BqVMMBdxJtkdd7zhQqKjo81lgQIFrrlPfHy82WxiYmJypGwicmv1ahpmmsOfnbYJ4Wfj8M7cnRi9cA+61Cthkn1ULBrk6CKK5O4adUpKCjp27Ihz585hxYoV19xvxIgRGDly5FX3q0Yt4h7iEpLxx6YIk4FrV2TaiXizCgXRu2lZs4Y4p3qJuEuN2mUCdf/+/TFv3jwTpK/3xq+sUUdERKBatWoK1CJuhj9daw5EmSbxBTsiTZM4lSpgbRZ/kM3iedQsLjkvVwbqQYMG4Y8//sDy5ctRtqx15aLMUh+1iPsLPxuLH9ccxtS1RxEdl2juy+PjZZrFn7yjHMoUDHR0ESUXCc9NgZpFGzx4MKZPn46lS5eiYsWKWX4OBWqR3NUsPoPN4isPmXzXxGbwTrWLY2CrCihfOK+jiyi5QHhuGkzGqVlTpkwxtWnOpY6MjDT358uXD3ny5HF08UTEyeTx9cLDjUrjoYalTLP418v3Y+nuU/h9YwSmb4pAh1rFMahVBZMIRMRVOHWNmmnwMjJx4kT07t07U8+hGrVI7rYl/Bw+W7QPf+20rmzGn5V7ahTDoJYVUa14sKOLJ24oPDfVqJ34HEJEXEStkvnxTa8G2H4sGp8v3od52yIxd6t1u6taUQxpVRE1S+ZzdDFFXHNlMhGR7FK9eD6Me7Q+5j9zBzrULm5q1lw/vMPnK9D3u3XYcPisKgfilJy66Ts7qOlbRDKy7+QFfLFkn5mTbZvaFVYwAPfVKm4CeeVi6seWG5OrRn1nBwVqEbmeg6cv4ssl+zBryzFcSkxbQ7xikbwmaN9XO1SjxSVLFKizSIFaRDLjYnySGXA2e8txLNt9CgnJaUG7amgw7qsVakaNly4Y4NByivNToM4iBWoRyarzlxKxcPsJU8tesfc0kmxt42ZwWj50rlvCJAQJ9HPq8bjiIArUWaRALSI34+zFBMzfHmlq2qv2n07tz+bypI/eVtokCykS5O/oYooTUaDOIgVqEckupy/EY86W4yYhCPu2ydfbEw/UK4Enbi+nvmwxFKizSIFaRLJbcorFTO3iymcbj5wz93G6111Vi+KpFuVQv8y1U/GK+wvPTQueiIg4I64ffneNYmhXvSjWHz6Lr5ftx187T2LBjhNma1AmBE+1KI/WVYrAUyk35SYpUIuI3MQyxw3DCpht38kYjF9+ADM2HjPBe/0P61G+cCD6NCtrBp9p4JncKDV9i4hkoxPnL2HiykOY/M9hxFxKMvfl9fM2/diP3lYGFZUQxO2Fq486axSoRcQRYi4l4uf14Zi05nDqwDO6rVwB9LwtDG2rF4WPl1Zxdkfh6qMWEXF+Qf4+eLx5WfRpGoZV+8/gxzWHzAA0pt/kViTIDw81Ko2HG5VCaD6l7ZVrU6AWEbmFOJisecVCZjt2Lg5T1x7BlLVHcTImHp8t2mvWG+docTaLNypbwEz3ErGnpm8RkRyWkJSCBTsi8ePqw/jnYFTq/b5enqgSGoQaJfKh5uWtUtEgBW8Xo6ZvEREXx8BrEn7UKo49J2JMP/bMzcdwLjYRW8KjzZa6r4J3rqcatYiIE+BPcfjZOBOkt0ZEY1uE9TI6LvGqfRmka5fMZxZWaRgWgvplQpA/wNch5ZarqUYtIuKmc7JLFQgwW/taoanB+2hUnAnYtuC9Jfwczl9KwrpDZ8321TLr31coktcstNIgrIC5LFMwwDynuD4FahERJ8VAy7Sa3OyDN6d7cVGVDQzWh6Nw4NRF7Dt5wWxT1x01+xXK64f6ZfKjSrFgFAj0Rf4AH1Przp/HByEBvsgX4INgf28FcxegQC0i4kIYWMsVzms2ptqkqIsJ2MDV0A5FmQC+NTzaJBCZv/2E2a63FCqzgJkgfjmAhwT6IiTA5/KlbUu7zX01/ztnKVCLiLg41pjvqlbUbHQpMdk0la8/dBZHomIRHZeAsxcTcS4uEediE8ygtbjEZJNchEGeW1YE+XunBnhepm2+qdftH+P+wf7WS28FefcM1F988QU+/PBDREZGonbt2hg7diwaNWrk6GKJiDglfx+v1DXIr4XBnAPVzl4O3AzgZ2Ott5mD21w3l2n3c38OP+bSqNw4+C2r8vh4WQP35QDOhWFsgTyvn5cpOzc/b09zmefybX8fzysuL2+X9+PGFoKs4IkKp8rFJyUjnpeJKcgfyC4BHzgTpw/U06ZNw3PPPYevvvoKjRs3xpgxY9CuXTvs3r0bRYoUcXTxRERcki24FQ32z1JgswV3XkbHJlovTU097bp1u7xPXKIJ6rEJyeY5WJPnxgVfspuPlwf8vb3gZxfQGfCTki0mGFuDsm1LRmLy1ZOe3n+gJro3LA1n4vSBevTo0ejXrx/69OljbjNgz5kzB99++y1efvllRxdPRCTXYI2VzezcsioxOQUXLtfEz19KNJutZn4+NZgnmZo+A/mlxBRz/VLS5cvUze52UooJvmmvYUFichJi4q3JULKClXEGdmfk1IE6ISEBGzZswPDhw1Pv8/T0RJs2bbB69eoM/yY+Pt5sNjExMTlSVhERuTYOQDMD0m4gyF9PSgpry7agfkUgT7TWnLloDOee+5naNi8vX+elj6d53Jn7zp06UJ8+fRrJyckoWtQ6QMKGt3ft2pXh34waNQojR47MoRKKiIij11LP4+tlNnflvKcQN4i17+jo6NRtx44dji6SiIiIe9aoCxUqBC8vL5w4kX4eIG8XK1Ysw7/x8/Mzm8358+dveTlFRERyZY3a19cX9evXx6JFi1LvS0lJMbebNGni0LKJiIggt9eoiVOzevXqhQYNGpi505yedfHixdRR4CIiIu7M6QN19+7dcerUKbz++utmwZM6dergzz//vGqAmYiIiDty+kBNgwYNMtuNYFM5HT9+PJtLJSIicjVbvLHFn1wRqG+GbSCalhwVEZGcxHzUpUvf/CpnHhbmTHNjSUlJ2Lhxo2kq52IpN4OLp1SrVs1M+QoKCoK7cff3lxveo7u/v9zwHt39/eWG9xgdHY0aNWrgzJkzKFDg2uutZ5bb16i9vb3RsGHDbHku21SvEiVKIDg4GO7G3d9fbniP7v7+csN7dPf3lxveY/Dl98T44/bTs0RERHI7BWoREREnpkCdBVzx7I033ki38pk7cff3lxveo7u/v9zwHt39/eWG9+iXze/P7QeTiYiIuDLVqEVERJyYArWIiIgTU6AWERFxYgrUmfTFF18gLCwM/v7+aNy4MdauXQt3MW7cONSqVcvM/ePGzGTz5s2DO4mIiMCjjz6KggULIk+ePKhZsybWr18Pd1tE4plnnkGZMmXMe2zatCnWrVsHV7R8+XJ06NABxYsXh4eHB2bMmJH6WGJiIoYNG2b+DwMDA80+jz32GI4dOwZ3eY/Uu3dvc7/9dvfdd8Nd3t+FCxfM0tAlS5Y0n1cugPLVV1/BVYwaNcqs0cEFW4oUKYL7778fu3fvTrfP+PHjceedd5rfVR6Dc+fO3dBrKVBnwrRp00wWL47i+/fff1G7dm20a9cOJ0+ehDvgF+W9997Dhg0bTPBq1aoVOnXqhO3bt8MdnD17Fs2aNYOPj485AeFqSB9//DFCQkLgTp544gksXLgQP/74I7Zu3Yq2bduiTZs25iTF1TBDHr9nPEG+UmxsrPkevvbaa+by999/Nz+QHTt2hLu8RxsGZq4bbdt++uknuMv7428qEyxNmjQJO3fuNCeZDNwzZ86EK1i2bBkGDhyINWvWmO8dTyD5neP7tv+s8v/w//7v/27uxTjqW66vUaNGloEDB6beTk5OthQvXtwyatQoi7sKCQmxfPPNNxZ3MGzYMEvz5s0t7iw2Ntbi5eVlmT17drr769WrZ3nllVcsrow/U9OnT7/uPmvXrjX7HT582OIu77FXr16WTp06WdxBRu+vevXqljfffNNtPq8nT54073PZsmVXPbZkyRLz2NmzZ2/ouVWj/g8JCQmmpsmaiQ3XDOft1atXw90kJydj6tSp5qyQTeDugGfozGf+4IMPmiaqunXrYsKECXC3Ne35f8euGXtsUlyxYoXDypWTayuzaTF//vxwJ0uXLjWf2cqVK6N///5m7Wh3wa4ZfjfZ4sNYvmTJEuzZs8fUSl31M0jZsbb3lRSo/8Pp06fND+CV+a95m/mx3QWbSvPmzWsm6D/99NOYPn266TNyBwcOHDD98BUrVsT8+fPND96QIUPw/fffw12wn4wnVm+99Zbpq+Vnlk2KPJl09xSvly5dMn3WDz/8sFutG80m0x9++AGLFi3C+++/b5pa77nnHvN/6w7Gjh1rfmPY9ebr62veL5vJ77jjDrialJQU03TPLjYm48hubp+UQzKHZ+ybNm0yZ4W//vorevXqZX4Y3CFY80vEGvW7775rbrNGvW3bNjNwhe/TXbBvum/fvibRgZeXF+rVq2eCF1uE3BX7Bbt162ZqZDwZcycPPfRQ6nUOnOOAz/Lly5taduvWreEOgZr9u6xVcwAkB5+xz5eDz+xbMF0By83flFvVeqUa9X8oVKiQ+dGz5bW24e1ixYrBXfCMtkKFCqhfv74ZzchBIJ9++incQWho6FUnHFWrVsWRI0fgTvgjzpMrjqZlHlzOTGAgK1euHNw5SB8+fNgM5nGn2nRG+P/I36N9+/bB1cXFxZkBVqNHjzYjw3kSwoFk3bt3x0cffQRXMmjQIMyePds03bN14FZQoM5EAGPwYvOTfQ2Nt92lDzcjfI/x8fFwB2yOunLaBPvCeBbvjjhliScnHO3Opn6O4HfXIL1371789ddfZtqduwsPDzd91Py/dYf/P24c72OPlSL+9rgCi8VigjS7CRcvXoyyZcvestdS03cmcBoBm0jZfNqoUSOMGTPGDLbq06cP3MHw4cNN31fp0qXNXNwpU6aY5jX+yLuDZ5991gxcYdM3f9xZ0+T8Rm7uhP9f/PFgNwZrXS+++CKqVKnikp9TtgrY1xwPHjxoumY4UIeBqmvXrmZqFmsy7LO1jRfh4zy5dvX3yG3kyJF44IEHTMvd/v378dJLL5lWL04NdfX3x9+aFi1amM8oBzzypJmtQeyTZy3bVZq7p0yZgj/++MOMEbF9BvPly2feE/E+brbjwLFA3JfvP0uDzrJhVHquMHbsWEvp0qUtvr6+ZrrWmjVrLO6ib9++ljJlypj3VrhwYUvr1q0tCxYssLiTWbNmWWrUqGHx8/OzVKlSxTJ+/HiLu5k2bZqlXLly5v+xWLFiZkrhuXPnLK7INp3lyo1Tlg4ePJjhY9z4d+7wHjndrm3btub76OPjY76f/fr1s0RGRlrc4f3R8ePHLb179zZTXf39/S2VK1e2fPzxx5aUlBSLK8A1PoMTJ05M3eeNN974z30yQ9mzREREnJj6qEVERJyYArWIiIgTU6AWERFxYgrUIiIiTkyBWkRExIkpUIuIiDgxBWoREREnpkAtIiLixBSoReSmMRf0jBkzHF0MEbekQC3i4nr37m0C5ZUb8/uKiOtTUg4RN8CgPHHixHT3+fn5Oaw8IpJ9VKMWcQMMysyyZL+FhISYx1i7HjdunMmQxqw+zGv866+/pvt7ZvVp1aqVeZwpI5988kmT/cjet99+i+rVq5vXYgYrpvizd/r0aXTu3BkBAQGoWLEiZs6cmfoYU2726NEDhQsXNq/Bx688sRCRjClQi+QCr732mkmZuHnzZhMwH3roIezcudM8xpStTJ3IwL5u3Tr88ssvJsezfSBmoGdaPwZwBnUGYaZctMe0jEwjumXLFtx7773mdaKiolJff8eOHZg3b555XT5foUKFcvgoiLio7E/+JSI5iWkDvby8LIGBgem2d955xzzOr/nTTz+d7m8aN25s6d+/v7nOlJ8hISGWCxcupD4+Z84ci6enZ2paRaYifOWVV65ZBr7Gq6++mnqbz8X75s2bZ2536NDB0qdPn2x+5yK5g/qoRdxAy5YtTS3Vnn1i+iZNmqR7jLc3bdpkrrOGW7t2bQQGBqY+3qxZM6SkpGD37t2m6fzYsWNo3br1dctQq1at1Ot8ruDgYJw8edLc7t+/v6nR//vvv2jbti3uv/9+NG3a9CbftUjuoEAt4gYYGK9sis4u7FPODB8fn3S3GeAZ7In944cPH8bcuXOxcOFCE/TZlP7RRx/dkjKLuBP1UYvkAmvWrLnqdtWqVc11XrLvmn3VNitXroSnpycqV66MoKAghIWFYdGiRTdVBg4k69WrFyZNmoQxY8Zg/PjxN/V8IrmFatQibiA+Ph6RkZHp7vP29k4dsMUBYg0aNEDz5s0xefJkrF27Fv/73//MYxz09cYbb5ggOmLECJw6dQqDBw9Gz549UbRoUbMP73/66adRpEgRUzuOiYkxwZz7Zcbrr7+O+vXrm1HjLOvs2bNTTxRE5PoUqEXcwJ9//mmmTNljbXjXrl2pI7KnTp2KAQMGmP1++uknVKtWzTzG6VTz58/H0KFD0bBhQ3Ob/cmjR49OfS4G8UuXLuGTTz7BCy+8YE4Aunbtmuny+fr6Yvjw4Th06JBpSr/99ttNeUTkv3lwRFkm9hMRF8W+4unTp5sBXCLietRHLSIi4sQUqEVERJyY+qhF3Jx6t0Rcm2rUIiIiTkyBWkRExIkpUIuIiDgxBWoREREnpkAtIiLixBSoRUREnJgCtYiIiBNToBYREXFiCtQiIiJwXv8PWQgBbiR/sacAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses = plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9d7f4",
   "metadata": {},
   "source": [
    "## 6-5 文本生成策略\n",
    "\n",
    "1. 温度缩放：温度越低越稳定，越高越随机\n",
    "2. top-k 采样：k 越小越稳定，越大越随机\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd09fe2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " I seemed to see it. Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"I seemed to see\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_length=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02365723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_length,\n",
    "        temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_length:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # 使用 top_k 采样筛选 logits\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits,\n",
    "            )\n",
    "        \n",
    "        if temperature > 0.0:\n",
    "            # 使用温度缩放\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            # 当不使用温度缩放时，执行贪心解码，选取下一个词元\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        \n",
    "        # 如果遇到序列结束词元，则提前停止生成\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc157264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " I seemed to see. And through my dear in the latter: \"interesting deprecating laugh\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"I seemed to see\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4,\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1193462e",
   "metadata": {},
   "source": [
    "## 5-6 权重保存和加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a21a597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dump_model_file = \"model_and_optimizer.pth\"\n",
    "\n",
    "try:\n",
    "    os.remove(dump_model_file)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "}, dump_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dec5dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " I seemed to see. And through my dear in the latter: \"interesting deprecating laugh\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(dump_model_file, map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=5e-4, weight_decay=0.1,\n",
    ")\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"I seemed to see\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4,\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df822010",
   "metadata": {},
   "source": [
    "## 5-7 加载 OpenAI 预训练权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7886b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n",
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\",\n",
    ")\n",
    "\n",
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())\n",
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "197381f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7df3b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    \"\"\"检查 2 个张量或数组是否具有相同的维度或形状，并将 right 张量返回为可训练的 PyTorch 参数\"\"\"\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb475b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    \"\"\"将从 GPT-2 的 params 字典中的权重加载到 GPTModel 的 gpt 实例的\"\"\"\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d0281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward\n",
      "Every effort moves you forward.\n",
      "Every effort moves you forward. \n",
      "Every effort moves you forward.  \n",
      "Every effort moves you forward.  The\n",
      "Every effort moves you forward.  The first\n",
      "Every effort moves you forward.  The first step\n",
      "Every effort moves you forward.  The first step is\n",
      "Every effort moves you forward.  The first step is to\n",
      "Every effort moves you forward.  The first step is to understand\n",
      "Every effort moves you forward.  The first step is to understand the\n",
      "Every effort moves you forward.  The first step is to understand the importance\n",
      "Every effort moves you forward.  The first step is to understand the importance of\n",
      "Every effort moves you forward.  The first step is to understand the importance of your\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work. \n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  \n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is to\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is to understand\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is to understand the\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "max_new_tokens = 25\n",
    "for i in range(max_new_tokens):\n",
    "    token_ids = generate(\n",
    "        model=gpt,\n",
    "        idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "        max_new_tokens=i+1,\n",
    "        context_length=NEW_CONFIG[\"context_length\"],\n",
    "        top_k=None,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    print(token_ids_to_text(token_ids, tokenizer).replace('\\n', \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6da96d",
   "metadata": {},
   "source": [
    "### 6 使用学习率预热、余弦衰减、梯度裁剪来优化训练过程\n",
    "\n",
    "- 学习率预热 (Warmup)：解决训练初期的不稳定性。\n",
    "- 余弦衰减 (Cosine Annealing)：解决训练中后期的精细调整和收敛问题。\n",
    "- 梯度裁剪 (Gradient Clipping)：解决训练过程中可能出现的梯度爆炸问题，充当“安全带”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86b93880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, device,\n",
    "                num_epochs, eval_freq, eval_iter, start_context, tokenizer,\n",
    "                warmup_steps, initial_lr=3e-05, min_lr=1e-6):\n",
    "    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    peak_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    total_training_steps = len(train_loader) * num_epochs # 计算训练过程中的所有迭代步数\n",
    "    lr_increment = (peak_lr - initial_lr) / warmup_steps  # 计算在预热阶段学习率的增量\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            global_step +=1\n",
    "\n",
    "            if global_step < warmup_steps:\n",
    "                lr = initial_lr + global_step * lr_increment    # 学习率预热阶段\n",
    "            else:\n",
    "                progress = ((global_step - warmup_steps) /\n",
    "                                    (total_training_steps - warmup_steps))\n",
    "                lr = min_lr + (peak_lr - min_lr) * 0.5 * (      # 余弦衰减阶段\n",
    "                    1 + math.cos(math.pi * progress))\n",
    "            for param_group in optimizer.param_groups:  # 在优化器上应用计算后的学习率\n",
    "                param_group[\"lr\"] = lr\n",
    "            track_lrs.append(lr)\n",
    "\n",
    "            # 计算交叉熵损伤并反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "\n",
    "            if global_step >= warmup_steps: # 在预热阶段后使用梯度裁剪来避免梯度爆炸\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), max_norm=1.0\n",
    "                )\n",
    "            \n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}):\"\n",
    "                    f\"Train loss {train_loss:.3f}, \"\n",
    "                    f\"Val loss {val_loss:.3f}\")\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen, track_lrs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
